{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Stock Recommendation System with Reinforcement Learning - RL Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import norm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "df = pd.read_csv(\"data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stock categories\n",
        "blue_chip_stocks = [\"AAPL\", \"MSFT\", \"JNJ\", \"V\", \"JPM\", \"PG\", \"UNH\", \"HD\", \"XOM\", \"KO\"]\n",
        "growth_stocks = [\"TSLA\", \"NVDA\", \"AMZN\", \"META\", \"SHOP\", \"NFLX\", \"AMD\", \"CRWD\", \"DDOG\", \"PLTR\"]\n",
        "midcap_tech = [\"UBER\", \"ABNB\", \"ROKU\", \"ZS\", \"OKTA\", \"SNOW\", \"MDB\", \"NET\", \"DOCU\", \"BILL\"]\n",
        "dividend_value = [\"T\", \"VZ\", \"PFE\", \"CVX\", \"WMT\", \"MCD\", \"PEP\", \"MMM\", \"IBM\", \"KO\"]\n",
        "cyclical_industrials = [\"CAT\", \"DE\", \"BA\", \"HON\", \"GE\", \"F\", \"GM\", \"LMT\", \"NOC\", \"UPS\"]\n",
        "\n",
        "# Initial style mapping\n",
        "raw_styles = {\n",
        "    \"growth\": growth_stocks + [\"GE\", \"F\", \"GM\"],\n",
        "    \"value\": blue_chip_stocks + [\"CAT\", \"HON\"],\n",
        "    \"conservative\": dividend_value + [\"MMM\", \"UPS\"],\n",
        "    \"trader\": midcap_tech + [\"DE\", \"BA\", \"LMT\", \"NOC\"]\n",
        "}\n",
        "\n",
        "valid_stocks = set(df.columns)\n",
        "styles = {\n",
        "    style: [s for s in stock_list if s in valid_stocks]\n",
        "    for style, stock_list in raw_styles.items()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_eval = df[df[\"Date\"] >= \"2025-01-01\"].copy()\n",
        "all_stocks = df['Stock'].unique().tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Define Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ------------------------\n",
        "# Evaluation: Simulated Reward / Sharpe\n",
        "# ------------------------\n",
        "def evaluate_sharpe(returns):\n",
        "    returns = np.array(returns)\n",
        "    if len(returns) == 0 or np.std(returns) == 0:\n",
        "        return 0.0\n",
        "    return np.mean(returns) / np.std(returns)\n",
        "\n",
        "def preference_alignment_score(recommended_stock, current_style, styles):\n",
        "    \"\"\"Score whether the recommended stock aligns with current user preference\"\"\"\n",
        "    return 1 if recommended_stock in styles[current_style] else 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Training and Evaluating RL Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ------------------------\n",
        "# Reinforcement Learning Environment Setup\n",
        "# ------------------------\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import random\n",
        "\n",
        "class StockRecommendationEnv(gym.Env):\n",
        "    def __init__(self, df, stock_list, styles):\n",
        "        super(StockRecommendationEnv, self).__init__()\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.stock_list = stock_list\n",
        "        self.styles = styles\n",
        "        self.num_stocks = len(stock_list)\n",
        "\n",
        "        self.action_space = spaces.Discrete(self.num_stocks)\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(8,), dtype=np.float32)\n",
        "\n",
        "        self.current_index = 0\n",
        "        self.max_index = len(self.df) - 2\n",
        "        self.risk_preference_map = {\"growth\": 1.2, \"value\": 1.0, \"conservative\": 0.8, \"trader\": 1.5}\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        self.current_index = 0\n",
        "        state = self._get_state()\n",
        "        return state, {}\n",
        "\n",
        "    def _get_state(self):\n",
        "        row = self.df.iloc[self.current_index]\n",
        "        style = row[\"Style\"]\n",
        "        risk_pref = self.risk_preference_map[style]\n",
        "        state =  np.array([\n",
        "            row[\"Daily_Return\"], row[\"Volatility\"], row[\"Momentum\"],\n",
        "            row[\"MA_10\"], row[\"Volatility_10\"], row[\"Shares\"],\n",
        "            row[\"Price\"], risk_pref\n",
        "        ], dtype=np.float32)\n",
        "        return np.nan_to_num(state, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "\n",
        "    def step(self, action):\n",
        "        row = self.df.iloc[self.current_index]\n",
        "        style = row[\"Style\"]\n",
        "        chosen_stock = self.stock_list[action]\n",
        "        next_price_row = self.df[(self.df.Stock == chosen_stock) & (self.df.index > self.current_index)].head(1)\n",
        "\n",
        "        if next_price_row.empty:\n",
        "            reward = 0\n",
        "            alignment = 0\n",
        "        else:\n",
        "            price_now = row[\"Price\"]\n",
        "            price_next = next_price_row[\"Price\"].values[0]\n",
        "            return_component = (price_next - price_now) * self.risk_preference_map[style]\n",
        "            alignment = 1 if chosen_stock in self.styles[style] else 0\n",
        "            alpha, beta = 0.5, 0.5\n",
        "            reward = alpha * return_component + beta * alignment\n",
        "\n",
        "        self.current_index += 1\n",
        "        done = self.current_index >= self.max_index\n",
        "        state = self._get_state() if not done else np.zeros(self.observation_space.shape)\n",
        "\n",
        "        return state, reward, done, False, {\"alignment\": alignment}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ------------------------\n",
        "# PPO Setup\n",
        "# ------------------------\n",
        "# Actor-Critic\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_size=64):\n",
        "        super().__init__()\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, action_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        probs = self.actor(x)\n",
        "        value = self.critic(x)\n",
        "        return probs, value\n",
        "\n",
        "# Collect Trajectories\n",
        "def collect_trajectories(env, policy, timesteps_per_batch):\n",
        "    state, _ = env.reset()\n",
        "    buffer = {\"states\": [], \"actions\": [], \"log_probs\": [], \"values\": [], \"rewards\": [], \"dones\": []}\n",
        "    for _ in range(timesteps_per_batch):\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "        probs, value = policy(state_tensor)\n",
        "        dist = torch.distributions.Categorical(probs)\n",
        "        action = dist.sample()\n",
        "\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
        "        done = terminated or truncated\n",
        "\n",
        "        buffer[\"states\"].append(state_tensor.squeeze(0))\n",
        "        buffer[\"actions\"].append(action)\n",
        "        buffer[\"log_probs\"].append(dist.log_prob(action))\n",
        "        buffer[\"values\"].append(value.squeeze(0))\n",
        "        buffer[\"rewards\"].append(torch.tensor([reward]))\n",
        "        buffer[\"dones\"].append(torch.tensor([done]))\n",
        "\n",
        "        state = next_state if not done else env.reset()\n",
        "    return buffer\n",
        "\n",
        "# GAE\n",
        "def compute_gae(rewards, values, dones, gamma=0.99, lam=0.95):\n",
        "    advantages, returns = [], []\n",
        "    gae = 0\n",
        "    values = values + [torch.tensor([0.0])]\n",
        "    for t in reversed(range(len(rewards))):\n",
        "        done_mask = 1.0 - dones[t].float()\n",
        "        delta = rewards[t] + gamma * values[t+1] * done_mask - values[t]\n",
        "        gae = delta + gamma * lam * done_mask * gae\n",
        "        advantages.insert(0, gae)\n",
        "    returns = [adv + val for adv, val in zip(advantages, values[:-1])]\n",
        "    return advantages, returns\n",
        "\n",
        "# PPO Update\n",
        "def ppo_update(policy, optimizer, states, actions, log_probs_old, returns, advantages,\n",
        "               clip_eps=0.2, critic_coef=0.5, entropy_coef=0.01):\n",
        "    states = torch.stack(states).detach()\n",
        "    actions = torch.stack(actions).detach()\n",
        "    old_log_probs = torch.stack(log_probs_old).detach()\n",
        "    returns = torch.stack(returns).detach()\n",
        "    advantages = torch.stack(advantages).detach()\n",
        "\n",
        "    for _ in range(5):  # epoch\n",
        "        probs, values = policy(states)\n",
        "        dist = torch.distributions.Categorical(probs)\n",
        "        log_probs = dist.log_prob(actions)\n",
        "        entropy = dist.entropy().mean()\n",
        "\n",
        "        ratio = torch.exp(log_probs - old_log_probs)\n",
        "        surr1 = ratio * advantages\n",
        "        surr2 = torch.clamp(ratio, 1 - clip_eps, 1 + clip_eps) * advantages\n",
        "        actor_loss = -torch.min(surr1, surr2).mean()\n",
        "        critic_loss = (returns - values.squeeze()).pow(2).mean()\n",
        "        loss = actor_loss + critic_coef * critic_loss - entropy_coef * entropy\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return actor_loss.item(), critic_loss.item()\n",
        "\n",
        "# Train PPO\n",
        "def train_ppo(env, state_dim, action_dim, total_timesteps=100000, batch_size=2048, lr=2e-4):\n",
        "    policy = ActorCritic(state_dim, action_dim)\n",
        "    optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
        "    timestep = 0\n",
        "    while timestep < total_timesteps:\n",
        "        buffer = collect_trajectories(env, policy, batch_size)\n",
        "        advs, rets = compute_gae(buffer[\"rewards\"], buffer[\"values\"], buffer[\"dones\"])\n",
        "        actor_loss, critic_loss = ppo_update(policy, optimizer,\n",
        "                                    buffer[\"states\"], buffer[\"actions\"], buffer[\"log_probs\"],\n",
        "                                    rets, advs\n",
        "                                )\n",
        "        timestep += batch_size\n",
        "        print(f\"Step {timestep} | Actor Loss: {actor_loss:.4f} | Critic Loss: {critic_loss:.4f}\")\n",
        "    return policy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Single-Agent RL: Scalar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Single-Agent RL Training & Evaluation\n",
        "# -----------------------------\n",
        "def train_and_evaluate_rl(df, stock_list, styles, holding_days=5):\n",
        "    train_df = df[df[\"Date\"] < \"2025-01-01\"].copy()\n",
        "    test_df = df[df[\"Date\"] >= \"2025-01-01\"].copy()\n",
        "    env = StockRecommendationEnv(train_df, stock_list, styles)\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "    model = train_ppo(env, state_dim, action_dim, total_timesteps=100000)\n",
        "    eval_env = StockRecommendationEnv(test_df, stock_list, styles)\n",
        "    state, _ = eval_env.reset()\n",
        "    returns, alignments = [], []\n",
        "    while True:\n",
        "        action, _ = model.predict(state)\n",
        "        next_state, reward, terminated, truncated, info = eval_env.step(action.item())\n",
        "        done = terminated or truncated\n",
        "        returns.append(reward)\n",
        "        alignments.append(info.get(\"alignment\", 0))\n",
        "        if done:\n",
        "            break\n",
        "    return np.mean(returns), evaluate_sharpe(returns), np.mean(alignments)\n",
        "\n",
        "df = df.dropna(subset=[\"Price\", \"Shares\"]).copy()\n",
        "ret_rl, sharpe_rl, align_rl = train_and_evaluate_rl(df, all_stocks, styles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"[Single-RL Agent]\")\n",
        "print(f\"Average Return: {ret_rl:.4f}, Sharpe: {sharpe_rl:.4f}, Alignment: {align_rl:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Single-Agent RL: One-hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ------------------------\n",
        "# (One-hot) Reinforcement Learning Environment Setup\n",
        "# ------------------------\n",
        "STYLE_TO_INDEX = {\"conservative\": 0, \"value\": 1, \"growth\": 2, \"trader\": 3}\n",
        "\n",
        "class StockRecommendationEnv(gym.Env):\n",
        "    def __init__(self, df, stock_list, styles):\n",
        "        super(StockRecommendationEnv, self).__init__()\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.stock_list = stock_list\n",
        "        self.styles = styles\n",
        "        self.num_stocks = len(stock_list)\n",
        "\n",
        "        # 8 original features + 4 style one-hot\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(12,), dtype=np.float32)\n",
        "        self.action_space = spaces.Discrete(self.num_stocks)\n",
        "\n",
        "        self.current_index = 0\n",
        "        self.max_index = len(self.df) - 2\n",
        "        self.risk_preference_map = {\"growth\": 1.2, \"value\": 1.0, \"conservative\": 0.8, \"trader\": 1.5}\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        self.current_index = 0\n",
        "        return self._get_state(), {}\n",
        "\n",
        "    def _get_state(self):\n",
        "        row = self.df.iloc[self.current_index]\n",
        "        style = row[\"Style\"]\n",
        "        risk_pref = self.risk_preference_map[style]\n",
        "        style_index = STYLE_TO_INDEX[style]\n",
        "        style_one_hot = np.zeros(len(STYLE_TO_INDEX))\n",
        "        style_one_hot[style_index] = 1\n",
        "\n",
        "        core_features = np.array([\n",
        "            row[\"Daily_Return\"], row[\"Volatility\"], row[\"Momentum\"],\n",
        "            row[\"MA_10\"], row[\"Volatility_10\"], row[\"Shares\"],\n",
        "            row[\"Price\"], risk_pref\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "        state = np.concatenate([core_features, style_one_hot])\n",
        "        return np.nan_to_num(state, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "\n",
        "    def step(self, action):\n",
        "        row = self.df.iloc[self.current_index]\n",
        "        style = row[\"Style\"]\n",
        "        chosen_stock = self.stock_list[action]\n",
        "        next_price_row = self.df[(self.df.Stock == chosen_stock) & (self.df.index > self.current_index)].head(1)\n",
        "\n",
        "        if next_price_row.empty:\n",
        "            reward = 0\n",
        "            alignment = 0\n",
        "        else:\n",
        "            price_now = row[\"Price\"]\n",
        "            price_next = next_price_row[\"Price\"].values[0]\n",
        "            return_component = (price_next - price_now) * self.risk_preference_map[style]\n",
        "            alignment = 1 if chosen_stock in self.styles[style] else 0\n",
        "            alpha, beta = 0.5, 0.5\n",
        "            reward = alpha * return_component + beta * alignment\n",
        "\n",
        "        self.current_index += 1\n",
        "        done = self.current_index >= self.max_index\n",
        "        next_state = self._get_state() if not done else np.zeros(self.observation_space.shape)\n",
        "\n",
        "        return next_state, reward, done, False, {\"alignment\": alignment}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Single-Agent RL Training & Evaluation\n",
        "# -----------------------------\n",
        "def train_and_evaluate_rl(df, stock_list, styles, holding_days=5):\n",
        "    train_df = df[df[\"Date\"] < \"2025-01-01\"].copy()\n",
        "    test_df = df[df[\"Date\"] >= \"2025-01-01\"].copy()\n",
        "    env = StockRecommendationEnv(train_df, stock_list, styles)\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "    model = train_ppo(env, state_dim, action_dim, total_timesteps=100000)\n",
        "    eval_env = StockRecommendationEnv(test_df, stock_list, styles)\n",
        "    state, _ = eval_env.reset()\n",
        "    returns, alignments = [], []\n",
        "    while True:\n",
        "        action, _ = model.predict(state)\n",
        "        next_state, reward, terminated, truncated, info = eval_env.step(action.item())\n",
        "        done = terminated or truncated\n",
        "        returns.append(reward)\n",
        "        alignments.append(info.get(\"alignment\", 0))\n",
        "        if done:\n",
        "            break\n",
        "    return np.mean(returns), evaluate_sharpe(returns), np.mean(alignments)\n",
        "\n",
        "df = df.dropna(subset=[\"Price\", \"Shares\"]).copy()\n",
        "ret_rl, sharpe_rl, align_rl = train_and_evaluate_rl(df, all_stocks, styles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"[Single-RL Agent]\")\n",
        "print(f\"Average Return: {ret_rl:.4f}, Sharpe: {sharpe_rl:.4f}, Alignment: {align_rl:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Multi-Agent RL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Multi-Agent RL Training & Evaluation\n",
        "# -----------------------------\n",
        "def train_multiagent_by_style(train_df, test_df, styles, stock_list, total_timesteps=200000):\n",
        "    agents = {}\n",
        "    results = {}\n",
        "\n",
        "    for style in styles.keys():\n",
        "        # print(f\"\\n[Training Agent for style: {style}]\")\n",
        "        train_style_df = train_df[train_df[\"Style\"] == style].dropna().copy()\n",
        "        test_style_df = test_df[test_df[\"Style\"] == style].dropna().copy()\n",
        "        if len(train_style_df) < 100:\n",
        "            # print(f\"Skipping {style} (too few samples)\")\n",
        "            continue\n",
        "\n",
        "        env = StockRecommendationEnv(train_df, stock_list, styles)\n",
        "        state_dim = env.observation_space.shape[0]\n",
        "        action_dim = env.action_space.n\n",
        "        model = train_ppo(env, state_dim, action_dim, total_timesteps=100000)\n",
        "        \n",
        "        agents[style] = model\n",
        "\n",
        "        eval_env = StockRecommendationEnv(test_df, stock_list, styles)\n",
        "        state, _ = eval_env.reset()\n",
        "        returns, alignments = [], []\n",
        "\n",
        "        while True:\n",
        "            action, _ = model.predict(state)\n",
        "            next_state, reward, terminated, truncated, info = eval_env.step(action.item())\n",
        "            done = terminated or truncated\n",
        "            returns.append(reward)\n",
        "            alignments.append(info.get(\"alignment\", 0))\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        avg_ret = np.mean(returns)\n",
        "        sharpe = evaluate_sharpe(returns)\n",
        "        align = np.mean(alignments)\n",
        "        results[style] = (avg_ret, sharpe, align)\n",
        "        # print(f\"Style {style}: Return {avg_ret:.4f}, Sharpe {sharpe:.4f}, Alignment {align:.4f}\")\n",
        "        \n",
        "    return agents, results\n",
        "\n",
        "train_df = df[df[\"Date\"] < \"2025-01-01\"].copy()\n",
        "test_df = df[df[\"Date\"] >= \"2025-01-01\"].copy()\n",
        "agents, results = train_multiagent_by_style(train_df, test_df, styles, all_stocks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"[Multi-Agent RL]\")\n",
        "for style, (ret, sharpe, align) in results.items():\n",
        "    print(f\"Style {style}: Average Return: {ret:.4f}, Sharpe: {sharpe:.4f}, Alignment: {align:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "weights = {\n",
        "    style: len(test_df[test_df[\"Style\"] == style]) \n",
        "    for style in results.keys()\n",
        "}\n",
        "total = sum(weights.values())\n",
        "\n",
        "ret_avg = sum(results[style][0] * weights[style] for style in results) / total\n",
        "sharpe_avg = sum(results[style][1] * weights[style] for style in results) / total\n",
        "align_avg = sum(results[style][2] * weights[style] for style in results) / total\n",
        "print(\"[Weighted Multi-Agent RL]\")\n",
        "print(f\"Average Return: {ret_avg:.4f}, Sharpe: {sharpe_avg:.4f}, Alignment: {align_avg:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
